{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import wrappers, Client\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = Client()\n",
    "openai_client = wrappers.wrap_openai(OpenAI())\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"2025년에 LangGraph 에이전트를 가장 많이 채택한 회사들은 어디인가요?\",\n",
    "        \"answer\": \"Uber, LinkedIn, Replit과 같은 선도 기업들은 강력할 뿐만 아니라 신뢰성까지 갖춘 에이전트를 구축하기 위해 LangGraph를 선택하고 있습니다. :cite[3]\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"AppFolio는 LangGraph를 사용하면서 어떤 개선을 이루었나요?\",\n",
    "        \"answer\": \"AppFolio는 부동산 관리자의 업무 시간 을 주당 10시간 이상 절감해 주는 코파일럿을 개발했고 , LangGraph는 앱 지연 시간을 줄이고 의사 결정의 정확도를 2배 높이는 데 도움을 주었습니다. :cite[4]\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"컨텍스트 엔지니어링은 왜 중요한가요?\",\n",
    "        \"answer\": \"LangGraph에서 컨텍스트 엔지니어링(Context Engineering)이 중요한 이유는, LangGraph가 에이전트 기반의 LLM 워크플로우를 정의하고 실행하는 시스템이기 때문입니다. 이 구조에서는 각 노드(에이전트)가 이전 상태를 바탕으로 행동하게 되며, 이때 전달되는 컨텍스트가 각 에이전트의 판단과 출력을 결정짓는 핵심 요소가 됩니다. :cite[2]:cite[5]\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Anthropic 팀이 컨텍스트 엔지니어링에 상당한 시간을 투자했음을 어떻게 알 수 있습니까?\",\n",
    "        \"answer\": \"장거리 대화 관리.  프로덕션 에이전트는 종종 수백 턴에 걸쳐 대화에 참여하기 때문에 신중한 맥락 관리 전략이 필요합니다. 대화가 확장됨에 따라 표준 맥락 창이 부족해지면서 지능형 압축 및 메모리 메커니즘이 필요하게 됩니다. :cite[2]:cite[7]\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Elastic의 LangGraph 구현은 무엇이 다른가요?\",\n",
    "        \"answer\": \"Elastic은 보안 중심 자동화 기능과 Elasticsearch 기반 RAG 에이전트 구현을 위한 종합 템플릿/툴킷을 제공함으로써, 일반적인 LangGraph 사용보다 도메인 특화, 검색 최적화, 운영 강조 측면에서 확연히 구분됩니다.  :cite[3]\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "inputs = [{\"question\": example[\"question\"]} for example in examples]\n",
    "outputs = [{\"answer\": example[\"answer\"]} for example in examples]\n",
    "\n",
    "# LangSmith의 Dataset을 생성\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=\"langgraph-blogs-qa\", description=\"LangGraph blogs QA.\"\n",
    ")\n",
    "\n",
    "# dataset에 example 데이터 추가\n",
    "client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)\n",
    "\n",
    "print(\n",
    "    f\"Dataset created in langsmith with ID: {dataset.id}\\n Navigate to {dataset.url}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.schema import Document\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_community.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        scraped_documents: list of documents\n",
    "        vectorstore: vectorstore\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    scraped_documents: List[str]\n",
    "    vectorstore: InMemoryVectorStore\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def scrape_blog_posts(state) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Scrape the blog posts and create a list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    urls = [\n",
    "        \"https://blog.langchain.com/is-langgraph-used-in-production/\",\n",
    "        \"https://blog.langchain.com/the-rise-of-context-engineering/\",\n",
    "        \"https://blog.langchain.com/why-langgraph-platform/\",\n",
    "    ]\n",
    "\n",
    "    docs = [WebBaseLoader(url).load() for url in urls]\n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "    return {\"scraped_documents\": docs_list}\n",
    "\n",
    "\n",
    "def indexing(state):\n",
    "    \"\"\"\n",
    "    Index the documents\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=250, chunk_overlap=0\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(state[\"scraped_documents\"])\n",
    "\n",
    "    # Add to vectorDB\n",
    "    vectorstore = InMemoryVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=OpenAIEmbeddings(),\n",
    "    )\n",
    "    return {\"vectorstore\": vectorstore}\n",
    "\n",
    "\n",
    "def retrieve_and_generate(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore and generate answer\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    vectorstore = state[\"vectorstore\"]\n",
    "\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # fetch relevant documents\n",
    "    docs = retriever.invoke(question)  # format prompt\n",
    "    formatted = prompt.invoke(\n",
    "        {\"context\": docs, \"question\": question})  # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "\n",
    "# Graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve_and_generate\", retrieve_and_generate)  # retrieve\n",
    "workflow.add_node(\"scrape_blog_posts\", scrape_blog_posts)  # scrape web\n",
    "workflow.add_node(\"indexing\", indexing)  # index\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"scrape_blog_posts\")\n",
    "workflow.add_edge(\"scrape_blog_posts\", \"indexing\")\n",
    "workflow.add_edge(\"indexing\", \"retrieve_and_generate\")\n",
    "\n",
    "workflow.add_edge(\"retrieve_and_generate\", END)\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import Client, evaluate, aevaluate\n",
    "from langsmith.evaluation import EvaluationResults\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "\n",
    "load_dotenv()\n",
    "client = Client()\n",
    "\n",
    "DEFAULT_DATASET_NAME = \"langgraph-blogs-qa\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "EVALUATION_PROMPT = f\"\"\"당신은 퀴즈를 채점하는 선생님입니다.\n",
    "\n",
    "                        질문, 정답(정답), 학생 답안이 주어집니다.\n",
    "\n",
    "                        채점 기준은 다음과 같습니다.\n",
    "                        (1) 정답 대비 사실적 정확성만을 기준으로 학생 답안을 채점합니다.\n",
    "                        (2) 학생 답안에 상충되는 진술이 없는지 확인합니다.\n",
    "                        (3) 정답 대비 사실적 정확성을 유지하는 한, 학생 답안에 정답보다 더 많은 정보가 포함되어 있어도 괜찮습니다.\n",
    "\n",
    "                        정확성(Correctness):\n",
    "                        참은 학생의 답안이 모든 기준을 충족함을 의미합니다.\n",
    "                        거짓은 학생의 답안이 모든 기준을 충족하지 못함을 의미합니다.\n",
    "\n",
    "                        추론과 결론이 정확한지 확인하기 위해 단계별로 추론 과정을 설명하십시오.\"\"\"\n",
    "\n",
    "# LLM-as-judge output schema\n",
    "\n",
    "class Grade(TypedDict):\n",
    "    \"\"\"Compare the expected and actual answers and grade the actual answer.\"\"\"\n",
    "    reasoning: Annotated[str, ...,\n",
    "                         \"실제 응답이 맞는지 아닌지에 대한 추론을 설명하세요.\"]\n",
    "    is_correct: Annotated[bool, ...,\n",
    "                          \"학생의 응답이 대부분 또는 정확히 맞으면 참이고, 그렇지 않으면 거짓입니다.\"]\n",
    "\n",
    "\n",
    "grader_llm = llm.with_structured_output(Grade)\n",
    "\n",
    "\n",
    "def transform_dataset_inputs(inputs: dict) -> dict:\n",
    "    \"\"\"Transform LangSmith dataset inputs to match the agent's input schema before invoking the agent.\"\"\"\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def transform_agent_outputs(outputs: dict) -> dict:\n",
    "    \"\"\"Transform agent outputs to match the LangSmith dataset output schema.\"\"\"\n",
    "    return {\"info\": outputs[\"info\"]}\n",
    "\n",
    "\n",
    "# Evaluator function\n",
    "async def evaluate_agent(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Evaluate if the final response is equivalent to reference response.\"\"\"\n",
    "    \n",
    "    user = f\"\"\"QUESTION: {inputs['question']}\n",
    "                GROUND TRUTH RESPONSE: {reference_outputs['answer']}\n",
    "                STUDENT RESPONSE: {outputs['answer']}\"\"\"\n",
    "\n",
    "    grade = await grader_llm.ainvoke([{\"role\": \"system\", \"content\": EVALUATION_PROMPT}, {\"role\": \"user\", \"content\": user}])\n",
    "    is_correct = grade[\"is_correct\"]\n",
    "    return is_correct\n",
    "\n",
    "\n",
    "# Target function\n",
    "async def run_graph(inputs: dict) -> dict:\n",
    "    \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\"\n",
    "    result = await graph.ainvoke({\n",
    "        \"question\": inputs[\"question\"]\n",
    "    })\n",
    "    return {\"answer\": result[\"answer\"].content}\n",
    "\n",
    "\n",
    "# run evaluation\n",
    "async def run_eval(\n",
    "    dataset_name: str,\n",
    "    experiment_prefix: Optional[str] = None,\n",
    ") -> EvaluationResults:\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    results = await aevaluate(\n",
    "        run_graph,\n",
    "        data=dataset,\n",
    "        evaluators=[evaluate_agent],\n",
    "        experiment_prefix=experiment_prefix,\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "async def main():\n",
    "    experiment_results = await run_eval(dataset_name=DEFAULT_DATASET_NAME,\n",
    "                                        experiment_prefix=\"langgraph-blogs-qa-evals2\")\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    import asyncio\n",
    "#    asyncio.run(main())\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langsmith import evaluate\n",
    "\n",
    "# See the prompt: https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-2\n",
    "prompt = hub.pull(\"langchain-ai/pairwise-evaluation-2\")\n",
    "model = init_chat_model(\"gpt-4o-mini\")\n",
    "chain = prompt | model\n",
    "\n",
    "def ranked_preference(inputs: dict, outputs: list[dict]) -> list:\n",
    "    # Assumes example inputs have a 'question' key and experiment\n",
    "    # outputs have an 'answer' key.\n",
    "    response = chain.invoke({\n",
    "        \"question\": inputs[\"question\"],\n",
    "        \"answer_a\": outputs[0].get(\"answer\", \"N/A\"),\n",
    "        \"answer_b\": outputs[1].get(\"answer\", \"N/A\"),\n",
    "    })\n",
    "\n",
    "    if response[\"Preference\"] == 1:\n",
    "        scores = [1, 0]\n",
    "    elif response[\"Preference\"] == 2:\n",
    "        scores = [0, 1]\n",
    "    else:\n",
    "        scores = [0, 0]\n",
    "    return scores\n",
    "\n",
    "evaluate(\n",
    "    (\"langgraph-blogs-qa-evals-9aa78b29\", \"langgraph-blogs-qa-evals2-5031643f\"),  # Replace with the names/IDs of your experiments\n",
    "    evaluators=[ranked_preference],\n",
    "    randomize_order=True,\n",
    "    max_concurrency=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
