{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in c:\\users\\moonj\\anaconda3\\lib\\site-packages (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created in langsmith with ID: 2f20aae8-fe4b-48ff-a0d9-82642ab726c3\n",
      " Navigate to https://smith.langchain.com/o/131371ab-0bf2-4ad3-88cd-3b2d53ea1af9/datasets/2f20aae8-fe4b-48ff-a0d9-82642ab726c3.\n"
     ]
    }
   ],
   "source": [
    "from langsmith import wrappers, Client\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI\n",
    "\n",
    "client = Client()\n",
    "openai_client = wrappers.wrap_openai(OpenAI())\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"2024년에 LangGraph 에이전트를 가장 많이 채택한 회사들은 어디인가요?\",\n",
    "        \"answer\": \"2025년 LangGraph 주요 채택 기업은 Replit, Elastic, LinkedIn, AppFolio, Uber 등으로, 다양한 업무에 에이전트를 활용하고 있습니다. :cite[3]\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Replit은 LangGraph를 사용하면서 어떤 개선을 이루었나요?\",\n",
    "        \"answer\": \"Replit은 LangGraph 도입으로 멀티 에이전트 흐름을 안정적으로 관리하고, 디버깅과 사용자 상호작용 추적을 대폭 개선했습니다. :cite[4]\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"2025년 LLM 인프라 트렌드는 어떤 것이 있었나요?\",\n",
    "        \"answer\": \"2025년 LLM 인프라는 멀티모달 역량, 자율 에이전트, 실시간 정보 통합 추론, 효율 중심 소형 모델, 하이브리드/엣지 배포, 분산 학습 방식, 효율 및 지속가능성 중심 프레임워크 등 다방면에서 진화하고 있습니다. :cite[2]:cite[5]\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"LangGraph는 2024년과 비교해 에이전트 워크플로를 어떻게 개선했나요?\",\n",
    "        \"answer\": \"LangGraph는 에이전트 상태 제어, human‑in‑the‑loop 워크플로 강화, 동적 멀티‑에이전트 흐름 구성, 의미 기반 기억 검색, 정적 타입 안정성, 그리고 스트리밍 및 checkpoint 효율성 향상 등 다방면에서 에이전트 워크플로 설계와 운영을 크게 진화시켰습니다. :cite[2]:cite[7]\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Elastic의 LangGraph 구현은 무엇이 다른가요?\",\n",
    "        \"answer\": \"Elastic은 보안 중심 자동화 기능과 Elasticsearch 기반 RAG 에이전트 구현을 위한 종합 템플릿/툴킷을 제공함으로써, 일반적인 LangGraph 사용보다 도메인 특화, 검색 최적화, 운영 강조 측면에서 확연히 구분됩니다.  :cite[3]\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "inputs = [{\"question\": example[\"question\"]} for example in examples]\n",
    "outputs = [{\"answer\": example[\"answer\"]} for example in examples]\n",
    "\n",
    "# LangSmith의 Dataset을 생성\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=\"langgraph-blogs-qa\", description=\"LangGraph blogs QA.\"\n",
    ")\n",
    "\n",
    "# dataset에 example 데이터 추가\n",
    "client.create_examples(inputs=inputs, outputs=outputs, dataset_id=dataset.id)\n",
    "\n",
    "print(\n",
    "    f\"Dataset created in langsmith with ID: {dataset.id}\\n Navigate to {dataset.url}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.schema import Document\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langchain_community.vectorstores import InMemoryVectorStore\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        scraped_documents: list of documents\n",
    "        vectorstore: vectorstore\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    scraped_documents: List[str]\n",
    "    vectorstore: InMemoryVectorStore\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def scrape_blog_posts(state) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Scrape the blog posts and create a list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    urls = [\n",
    "        \"https://blog.langchain.dev/top-5-langgraph-agents-in-production-2024/\",\n",
    "        \"https://blog.langchain.dev/langchain-state-of-ai-2024/\",\n",
    "        \"https://blog.langchain.dev/introducing-ambient-agents/\",\n",
    "    ]\n",
    "\n",
    "    docs = [WebBaseLoader(url).load() for url in urls]\n",
    "    docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "    return {\"scraped_documents\": docs_list}\n",
    "\n",
    "\n",
    "def indexing(state):\n",
    "    \"\"\"\n",
    "    Index the documents\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=250, chunk_overlap=0\n",
    "    )\n",
    "    doc_splits = text_splitter.split_documents(state[\"scraped_documents\"])\n",
    "\n",
    "    # Add to vectorDB\n",
    "    vectorstore = InMemoryVectorStore.from_documents(\n",
    "        documents=doc_splits,\n",
    "        embedding=OpenAIEmbeddings(),\n",
    "    )\n",
    "    return {\"vectorstore\": vectorstore}\n",
    "\n",
    "\n",
    "def retrieve_and_generate(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore and generate answer\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    vectorstore = state[\"vectorstore\"]\n",
    "\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "    # fetch relevant documents\n",
    "    docs = retriever.invoke(question)  # format prompt\n",
    "    formatted = prompt.invoke(\n",
    "        {\"context\": docs, \"question\": question})  # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "\n",
    "# Graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve_and_generate\", retrieve_and_generate)  # retrieve\n",
    "workflow.add_node(\"scrape_blog_posts\", scrape_blog_posts)  # scrape web\n",
    "workflow.add_node(\"indexing\", indexing)  # index\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"scrape_blog_posts\")\n",
    "workflow.add_edge(\"scrape_blog_posts\", \"indexing\")\n",
    "workflow.add_edge(\"indexing\", \"retrieve_and_generate\")\n",
    "\n",
    "workflow.add_edge(\"retrieve_and_generate\", END)\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\moonj\\anaconda3\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1643: UserWarning: Cannot use method='json_schema' with model gpt-3.5-turbo since it doesn't support OpenAI's Structured Output API. You can see supported models here: https://platform.openai.com/docs/guides/structured-outputs#supported-models. To fix this warning, set `method='function_calling'. Overriding to method='function_calling'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'langgraph-blogs-qa-evals-8a9fc4b8' at:\n",
      "https://smith.langchain.com/o/131371ab-0bf2-4ad3-88cd-3b2d53ea1af9/datasets/2f20aae8-fe4b-48ff-a0d9-82642ab726c3/compare?selectedSessions=dc3c4fda-898c-4d08-bcc5-8c7fe0e6eec6\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3b30864f774f65a5e61cadf5b1c2c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import Client, evaluate, aevaluate\n",
    "from langsmith.evaluation import EvaluationResults\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "# from rag_graph import graph\n",
    "from dotenv import load_dotenv\n",
    "import nest_asyncio\n",
    "\n",
    "load_dotenv()\n",
    "client = Client()\n",
    "\n",
    "DEFAULT_DATASET_NAME = \"langgraph-blogs-qa\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "EVALUATION_PROMPT = f\"\"\"당신은 퀴즈를 채점하는 선생님입니다.\n",
    "\n",
    "                        질문, 정답(정답), 학생 답안이 주어집니다.\n",
    "\n",
    "                        채점 기준은 다음과 같습니다.\n",
    "                        (1) 정답 대비 사실적 정확성만을 기준으로 학생 답안을 채점합니다.\n",
    "                        (2) 학생 답안에 상충되는 진술이 없는지 확인합니다.\n",
    "                        (3) 정답 대비 사실적 정확성을 유지하는 한, 학생 답안에 정답보다 더 많은 정보가 포함되어 있어도 괜찮습니다.\n",
    "\n",
    "                        정확성(Correctness):\n",
    "                        참은 학생의 답안이 모든 기준을 충족함을 의미합니다.\n",
    "                        거짓은 학생의 답안이 모든 기준을 충족하지 못함을 의미합니다.\n",
    "\n",
    "                        추론과 결론이 정확한지 확인하기 위해 단계별로 추론 과정을 설명하십시오.\"\"\"\n",
    "\n",
    "# LLM-as-judge output schema\n",
    "\n",
    "\n",
    "class Grade(TypedDict):\n",
    "    \"\"\"Compare the expected and actual answers and grade the actual answer.\"\"\"\n",
    "    reasoning: Annotated[str, ...,\n",
    "                         \"실제 응답이 맞는지 아닌지에 대한 추론을 설명하세요.\"]\n",
    "    is_correct: Annotated[bool, ...,\n",
    "                          \"학생의 응답이 대부분 또는 정확히 맞으면 참이고, 그렇지 않으면 거짓입니다.\"]\n",
    "\n",
    "\n",
    "grader_llm = llm.with_structured_output(Grade)\n",
    "# PUBLIC API\n",
    "\n",
    "\n",
    "def transform_dataset_inputs(inputs: dict) -> dict:\n",
    "    \"\"\"Transform LangSmith dataset inputs to match the agent's input schema before invoking the agent.\"\"\"\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def transform_agent_outputs(outputs: dict) -> dict:\n",
    "    \"\"\"Transform agent outputs to match the LangSmith dataset output schema.\"\"\"\n",
    "    return {\"info\": outputs[\"info\"]}\n",
    "\n",
    "# Evaluator function\n",
    "\n",
    "\n",
    "async def evaluate_agent(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Evaluate if the final response is equivalent to reference response.\"\"\"\n",
    "    \n",
    "    user = f\"\"\"QUESTION: {inputs['question']}\n",
    "                GROUND TRUTH RESPONSE: {reference_outputs['answer']}\n",
    "                STUDENT RESPONSE: {outputs['answer']}\"\"\"\n",
    "\n",
    "    grade = await grader_llm.ainvoke([{\"role\": \"system\", \"content\": EVALUATION_PROMPT}, {\"role\": \"user\", \"content\": user}])\n",
    "    is_correct = grade[\"is_correct\"]\n",
    "    return is_correct\n",
    "\n",
    "\n",
    "# Target function\n",
    "async def run_graph(inputs: dict) -> dict:\n",
    "    \"\"\"Run graph and track the trajectory it takes along with the final response.\"\"\"\n",
    "    result = await graph.ainvoke({\n",
    "        \"question\": inputs[\"question\"]\n",
    "    })\n",
    "    return {\"answer\": result[\"answer\"].content}\n",
    "\n",
    "# run evaluation\n",
    "\n",
    "\n",
    "async def run_eval(\n",
    "    dataset_name: str,\n",
    "    experiment_prefix: Optional[str] = None,\n",
    ") -> EvaluationResults:\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    results = await aevaluate(\n",
    "        run_graph,\n",
    "        data=dataset,\n",
    "        evaluators=[evaluate_agent],\n",
    "        experiment_prefix=experiment_prefix,\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "async def main():\n",
    "    experiment_results = await run_eval(dataset_name=DEFAULT_DATASET_NAME,\n",
    "                                        experiment_prefix=\"langgraph-blogs-qa-evals\")\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    import asyncio\n",
    "#    asyncio.run(main())\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langsmith import evaluate\n",
    "\n",
    "# See the prompt: https://smith.langchain.com/hub/langchain-ai/pairwise-evaluation-2\n",
    "prompt = hub.pull(\"langchain-ai/pairwise-evaluation-2\")\n",
    "model = init_chat_model(\"gpt-4o-mini\")\n",
    "chain = prompt | model\n",
    "\n",
    "def ranked_preference(inputs: dict, outputs: list[dict]) -> list:\n",
    "    # Assumes example inputs have a 'question' key and experiment\n",
    "    # outputs have an 'answer' key.\n",
    "    response = chain.invoke({\n",
    "        \"question\": inputs[\"question\"],\n",
    "        \"answer_a\": outputs[0].get(\"answer\", \"N/A\"),\n",
    "        \"answer_b\": outputs[1].get(\"answer\", \"N/A\"),\n",
    "    })\n",
    "\n",
    "    if response[\"Preference\"] == 1:\n",
    "        scores = [1, 0]\n",
    "    elif response[\"Preference\"] == 2:\n",
    "        scores = [0, 1]\n",
    "    else:\n",
    "        scores = [0, 0]\n",
    "    return scores\n",
    "\n",
    "evaluate(\n",
    "    (\"langgraph-blogs-qa-evals-2-e5e3746f\", \"langgraph-blogs-qa-evals-2-e5e3746f\"),  # Replace with the names/IDs of your experiments\n",
    "    evaluators=[ranked_preference],\n",
    "    randomize_order=True,\n",
    "    max_concurrency=4,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
